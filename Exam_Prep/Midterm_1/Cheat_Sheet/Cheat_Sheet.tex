\documentclass{article}
\usepackage[left=3cm, right=3cm, top=3cm]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{color}
\begin{document}

\noindent{\Large Systems of Equations and Gaussian Elimination}
\begin{itemize}
	\item Gaussian elimination:
	\begin{itemize}
		\item No solution: a row of 0s sum to $\neq 0$ (Priority)
		\item Unique solution: For $n$ variables, we have $n$ pivots
		\item Infinite solutions: Fewer pivots than variables
	\end{itemize}
\end{itemize}

\noindent{\Large Linear Transformations and Linear Dependence}
\begin{itemize}
	\item {\color{red}!!! Care: dimensions in multiplication}
	\item Def. A standard unit vector, $\vec{e_1}, \vec{e_2}$ etc., is a vector with all components equal to 0 except for one element, which is equal to 1.
	\item Def. $\mathbf{A}_{ij}: i^{th}$ row and $j^{th}$ column of $\mathbf{A}$
	\item Def. Linear Dependence:
	A set of vectors ${\vec{v_1},...,\vec{v_n}}$ is linearly dependent
	\begin{itemize}
		\item $\iff$ There exist scalars $\alpha_1,...,\alpha_n$ such that $\sum\limits_{i=1}^n \alpha_i\vec{v_i} = \alpha_1\vec{v_1} + ... + \alpha_n\vec{v_n} = \vec{0}$ and not all $\alpha_i$'s are equal to zero. (Easier starting point mathematically.)
		\item $\iff$ There exist scalars $\alpha_1,...,\alpha_n$ and an index $i$ such that $\vec{v_i} = \sum\limits_{j\neq i} \alpha_j\vec{v_j}$.
		(In words, one of the vectors could be written as a linear combination of the rest of the vectors.)
	\end{itemize}
	\item Def. A set of vectors ${\vec{v_1},...,\vec{v_n}}$ is linearly independent $\iff \alpha_1\vec{v_1} + ... + \alpha_n\vec{v_n} = \vec{0}$ implies $\alpha_1 = ... = \alpha_n = 0$
	\item Theorem 3.1: If the system of linear equations $A\vec{x} = \vec{b}$ has infinite number of solutions, then the columns of $A$ are linearly dependent.
	\item Theorem 3.2 ($\approx$ converse): If the columns of $A$ in system $A\vec{x} = \vec{b}$ are linearly dependent, then the system does not have a unique solution (either no solution or infinite solutions).
	\item Theorem 3.3: If the system of linear equations $A\vec{x} = \vec{b}$ has an infinite number of solutions and the number of rows in $A\geq$ the number of columns ($A$ is a square or a tall matrix), then the rows of $A$ are linearly dependent.
	\item (Proved in HW): Let $n\in\mathbb{Z^+}$, and let $\{\vec{v_1},...,\vec{v_k}\}$ be a set of $k$ linearly dependent vectos in $\mathbb{R}^n.$ Then, for any $n$x$n$ matrix $A$, the set $\{\vec{Av_1},...,\vec{Av_k}\}$ is a set of linearly dependent vectors.
	\item {\color{red}!!! Rotation matrices} $R$ would rotate any vector by angle $\theta$ in the counterclockwise direction.
	\[ \mathbf{R} = 
	\begin{bmatrix}
    	cos(\theta) & -sin(\theta) \\
    	sin(\theta) & cos(\theta)
	\end{bmatrix}
	\]
	\begin{itemize}
		\item All rotation matrices in $n$ dimensions can be decomposed into a series of rotations in 2 dimensions. So all rotations are products of the basic rotation matrix $R$ generalized to a larger $n$x$n$ matrix with 1’s in the dimensions that aren’t being rotated. 
	\end{itemize}
	\item Reflection matrices: $R_1$ reflects across $x$-axis, $R_2$ reflects across $y$-axis, $R_3$ reflects across $y = x$, $R_4$ reflects across $y = -x$..
	\[ \mathbf{R_1} = 
	\begin{bmatrix}
    	1 & 0 \\
    	0 & -1
	\end{bmatrix},\ 
	\mathbf{R_2} = 
	\begin{bmatrix}
    	-1 & 0 \\
    	0 & 1
	\end{bmatrix},\ 
	\mathbf{R_3} = 
	\begin{bmatrix}
    	0 & 1 \\
    	1 & 0
	\end{bmatrix},\ 
	\mathbf{R_4} = 
	\begin{bmatrix}
    	0 & -1 \\
    	-1 & 0
	\end{bmatrix}
	\]
	\item $\vec{v_1}, \vec{v_2}$, and $\vec{v_1} + \vec{v_2}$ are all solutions to the system of linear equation $A\vec{x} = \vec{b}.$ Prove that $\vec{b} = \vec{0}.$
	\item {\color{red} Common mistake:} Two linearly independent vectors with 3 elements in them do not span $\mathbb{R}^2$.
\end{itemize}

\noindent{\Large State Transition Matrices and Inverses}
\begin{itemize}
	\item Calculations of solutions, inverses, null spaces: {\color{red}Check} by actually multiplying out the answer with the original matrix.
	\item Proved in lecture: The left and right inverses are identical.
	\item Proved in Notes: If $A$ is an invertible matrix, then its inverse must be unique.
	\item Theorem 6.1: A matrix $M$ is invertible $\iff$ its rows are linearly independent.
	\item Theorem 6.2: A matrix $M$ is invertible $\iff$ its columns are linearly independent.
	\item Invertible Matrix Theorem: $A$ is invertible
	\begin{itemize}
		\item $\iff$ Null$(A)=\vec{0}$
		\item $\iff$ The columns of A are linearly independent (Th. 6.2)
		\item $\iff$ The equation $A\vec{x} = \vec{0}$, has a unique solution, which is $\vec{x} = \vec{0}$
		\item $\iff$ For each column vector $\vec{b}\in\mathbb{R}^n, A\vec{x} = \vec{b}$ has a unique solution $\vec{x}.$
		\item $\iff$ $A$ does not have an eigenvalue $\lambda = 0$ (Can use directly?)
		\item $\iff$ The determinant of det$(A) \neq 0$
		\item $\iff$ The rank of $A$ is equal to its dimension
	\end{itemize}
	\item $(AB)^{-1} = B^{-1}A^{-1}$ since $(AB)(B^{-1}A^{-1}) = A(BB^{-1})A^{-1} = AIA^{-1} = AA^{-1} = I$
	\item If a state-transition matrix has a non-trivial nullspace, then it's non-invertible, so the information about previous states isn't preserved.
\end{itemize}

\noindent{\Large Vector Spaces}
\begin{itemize}
	\item The Basis for $\mathbb{R}^n$ must be exactly $n$ linearly independent vectors in $\mathbb{R}^n$. (Because Basis is defined as a \textbf{minimal}, spanning set of vectors for a given vector space)
	\item Proof requires:
	\begin{itemize}
		\item Closure 1 - scaling
		\item Closure 2 - additivity
		\item Contains $\vec{0}$
		\item Subset of $\mathbb{R}^n$
	\end{itemize}
\end{itemize}

\noindent{\Large Eigenvalues and PageRank (Diagonalization)}
\begin{itemize}
	\item (Proved in HW:) If an invertible matrix $\mathbf{A}$ has an eigenvalue $\lambda$, then $\mathbf{A}^{-1}$ has the eigenvalue $\frac{1}{\lambda}.$
	\item (Proved in HW:) If $\mathbf{A}$ has an eigenvalue $\lambda$, then $\mathbf{A}^T$ also has the eigenvalue $\lambda.$ Because of the fact that det$(\mathbf{A} - \lambda I) = $ det$(\mathbf{A}^T - \lambda I)$
	\item Every eigenvector can only correspond to one eigenvalue.
	\item A matrix with only real entries can have complex eigenvalues. $\big[[0, -1],\ [1, 0]\big]$
	\item The zero matrix only has 1 distinct eigenvalue. -$>$ A diagonal $n$x$n$ does NOT necessarily have $n$ distinct eigenvalues.
	\item (In Notes) An $n$x$n$ matrix is diagonalizable if it has $n$ linearly independent eigenvectors, (Copy down detailed diagonalization procedure in P38 of review guide); but if An $n$x$n$ matrix is invertible then it doesn't necessarily have $n$ linearly independent eigenvectors,
	\item {\color{red} Steady State:} An eigenvalue of 1 does not mean there is always a steady state. We also want the other eigenvalues to $<1$ in magnitude so that as $n$ goes to infinity, $\lambda^n$ goes to 0.
	\item Regarding determinant,
	\begin{itemize}
		\item det$(I_n) = 1$
		\item det$(A^T) = $ det$(A)$
		\item det$(A^{-1}) = \frac{1}{det(A)} = $ det$(A)^{-1}$
		\item det$(AB) = $ det$(A)\cdot$det$(B)$ for square matrices $A, B$ of the same size
		\item det$(cA) = c^n\cdot$det$(A)$ for an $n$x$n$ matrix
	\end{itemize}
	\item (Proved In Notes) If two $n$x$n$ diagonalizable matrices $A$ and $B$ have the same eigenvectors, then their matrix multiplication is commutative, i.e. $AB = BA$
	\item If $A$ and $B$ are $n$x$n$ matrices that share the same $n$ distinct eigenspaces, then $A$ and $B$ commute, that is, $AB = BA.$
\end{itemize}

\noindent{\Large Extra Sanity Checks}
\begin{itemize}
	\item $AA^{-1} = A^{-1}A = I$
	\item For matrix $A$, if $\lambda, \vec{v_1}$ is a eigenpair, then $A\vec{v_1} = \lambda\vec{v_1}$
	\item Non-trivial translation is not a linear transformation.
	\item For a state-transition matrix to converge, it has to have all $|\lambda|\leq1$
	\item In Notes: If $A$ is $n$x$n$, then the dimension of its column space + dimension of its nullspace = $n$
	\item Diagonalization: $A = P\Lambda P^{-1}$ where $\Lambda$ is a diagonal matrix of its eigenvalues, and $P$ is a matrix whose column vectors are the corresponding eigenvectors.
	\item Proofs are generally not expected to be ``hardcore''
	\begin{itemize}
		\item Past exams also usually have “direct” proofs rather than something like contradiction, etc.
		\item Oftentimes you can do it in a few lines with some explanation
		\item If your proof becomes a bit too unwieldy, try to start over with a different strategy
		\item Start with the fundamental equations about what you know. Try to slowly involve the things you care about showing
	\end{itemize}
\end{itemize}

\end{document}