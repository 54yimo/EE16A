\documentclass{article}
\usepackage[left=3cm, right=3cm, top=3cm]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{color}
\usepackage{graphicx}
\begin{document}

\noindent{\Large Circuit Design}
\begin{itemize}
	\item Design Procedure
	\begin{enumerate}
		\item Step 1 (Specification): Concretely restate the goals for the design.
		\item Step 2 (Strategy): Describe your strategy (often in the form of a block diagram) to achieve your goal.
		\item Step 3 (Implementation): Implement the components described in your strategy.
		\item Step 4 (Verification): Check that your design from Step 3 does what you specified in Step 1.
	\end{enumerate}
	\item ``Almost'' Current Source ($V_s$ + negative feedback op-amp)
\end{itemize}



\noindent{\Large Locationing and Trilateration}
\begin{itemize}
	\item Inner Product
	\begin{itemize}
		\item Commutative: $\langle \vec{x}, \vec{y} \rangle = \langle \vec{y}, \vec{x} \rangle$
		\item $\langle c\vec{x}, \vec{y} \rangle = c\langle \vec{x}, \vec{y} \rangle$
		\item Linearity: $\langle \vec{x}, \vec{y+z} \rangle = \langle \vec{x}, \vec{y} \rangle + \langle \vec{x}, \vec{z} \rangle$
	\end{itemize}
	\item Orthogonal $\iff$ Inner product = 0
	\item Euclidean Norm:
		$\Vert \vec{x} \Vert_2 =
			\sqrt{x_1^2 + \cdots + x_n^2} = \sqrt{\langle \vec{x}, \vec{x} \rangle} $
	\begin{itemize}
		\item Non-negativity, Zero Vector, Scalar Multiplication, Triangle Inequality
	\end{itemize}
	\item $\langle \vec{x}, \vec{y} \rangle =
		\Vert \vec{x} \Vert\, \Vert \vec{y} \Vert \cos\theta
		\Longrightarrow \vert \langle \vec{x}, \vec{y} \rangle \vert \leq
		\Vert \vec{x} \Vert\, \Vert \vec{y} \Vert$
		The Cauchy-Schwarz inequality

	\item The projection of $\vec{y}$ onto $\vec{x}$
		(with $\vec{u} = \vec{x}\, /\, \Vert \vec{x} \Vert$) is:
		$$\text{proj}_{\vec{x}} \vec{y} =
					\langle \vec{y}, \vec{u} \rangle \vec{u} =
					\Big\langle \vec{y}, \frac{\vec{x}}{\Vert\vec{x}\Vert} \Big\rangle\
						\frac{\vec{x}}{\Vert\vec{x}\Vert} =
		\frac{\langle \vec{y}, \vec{x}\rangle}
			{\langle \vec{x}, \vec{x} \rangle} \ \vec{x} $$
	\item {\color{blue} Linear cross-correlation}:
		corr$(\vec{x}, \vec{y})[k] =
			\sum\limits_{i=-\infty}^\infty \vec{x}[i]\,\vec{y}[i-k]$
	\item {\color{blue} Circular cross-correlation} with period $N$:
		circcorr$(\vec{x}, \vec{y})[k] =
			\sum\limits_{i=0}^{N-1} \vec{x}[i] \, \vec{y}[(i-k)_N]$ \\
		$\iff$ circcorr$(\vec{x}, \vec{y})[k] =
			\big\langle \vec{x}, \vec{y}^{(k)} \big\rangle =
				\vec{y}^{(k)^T} \vec{x} $ \\

		Thus, circcorr$(\vec{x}, \vec{y}) = \begin{bmatrix}
		\big\langle \vec{x}, \vec{y}^{(0)} \big\rangle \\
		\big\langle \vec{x}, \vec{y}^{(1)} \big\rangle \\
		\vdots \\
		\big\langle \vec{x}, \vec{y}^{(N-1)} \big\rangle
		\end{bmatrix} = C_{\vec{y}} \vec{x}$
		where $C_{\vec{y}}$ is the matrix containing all of the shifted versions of $\vec{y}$ in its rows.
	\item corr$(\vec{x}, \vec{y}) \neq$ corr$(\vec{y}, \vec{x})$, and thus,
		circcorr$(\vec{x}, \vec{y}) \neq$ circcorr$(\vec{y}, \vec{x})$
	\item {\color{blue} Autocorrelation}: a correlation between a signal and itself, i.e. circcorr$(\vec{x}, \vec{x})$
\end{itemize}



\noindent{\Large Least Squares}
\begin{itemize}
	\item Error vector: $\vec{e} = \mathbf{A}\vec{x} - \vec{b}$
	\item Minimizing $\Vert\vec{e}\Vert^2$, the optimal solution (in least squares sense) is:
		$\vec{\tilde{x}} = (\mathbf{A}^T \mathbf{A})^{-1}\ \mathbf{A}^T \vec{b}$

% \begin{figure} [h!]
% 	\begin{center}
% 	\includegraphics[width=0.5\linewidth]{/Users/Alan/Desktop/invert-or-not}
% 	\caption{Inverting v. Non-Inverting}
% 	\label{fig}
% 	\end{center}
% \end{figure}

\end{itemize}




\noindent{\Large OMP}
\begin{itemize}
	\item Setup:
	\begin{itemize}
		\item Let the number of unique codes be $m$. Each code is length $n$. We represent each of the $m$ codes with a vector $\vec{s_i}$ where $i\in\{0,\dots,m-1\}$. Each code can potentially carry a message $a_i$ along with it. Then the measurement at the receiver is:
			$$\vec{y} = a_0\vec{s}_0^{(\tau_0)} + a_1\vec{s}_1^{(\tau_1)} + \cdots +
				a_{m-1}\vec{s}_{m-1}^{(\tau_{m-1})}$$
		We will assume that the number of codes that are being broadcast at the same time is very small (for example, 10). If a code is not being broadcast, it’s coefficient $a_i = 0$. Therefore, there are at most $k$ non-zero $a_i$’s.
	\end{itemize}
	\item Inputs:
	\begin{itemize}
		\item A set of $m$ codes, each of length $n$, i.e. $S = \{\vec{s_0}, \vec{s_1}, \dots, \vec{s_{m-1}}\}$
		\item An $n$-dimensional received signal vector, i.e. $\vec{y}$
		\item The sparsity level $k$ of the signal, i.e. the number of codes with non-zero coefficients.
		\item Some threshold, $th$. When $\Vert \vec{e}\Vert < th$, the signal is assumed to contain only noise.
	\end{itemize}
	\item Outputs:
	\begin{itemize}
		\item A set of codes that were identified, $F$, which will contain at most $k$ elements.
		\item A vector $\vec{x}$ containing the coefficients of the codes ($a_1$ etc.), which will be of length $k$ or less.
		\item An $n$-dimensional residual $\vec{e}$
	\end{itemize}
	\item Procedure:
	\begin{itemize}
		\item Initialize the following values:
		$\vec{e} = \vec{y}, j=1, A = [\,], F = \{\emptyset\}$
		\item While $j\leq k$ and $\Vert \vec{e}\Vert \geq th$:
		\begin{enumerate}
			\item Cross correlate $\vec{e}$ with each of the codes. Find the code index, $i$, and the shifted version of the code, $\vec{s}_i^{(\tau_i)}$, with which the received signal has the highest correlation value.
			\item Add $i$ to the set of code indices, $F$.
			\item Column concatenate matrix $A$ with the correct shifted version of the code:
			$A = \big[A\ \big|\ \vec{s}_i^{(\tau_i)} \big]$
			\item Use least squares to obtain the code coefficients: $\vec{x} = (\mathbf{A}^T \mathbf{A})^{-1}\ \mathbf{A}^T \vec{y}$
			\item Update the residual value: $\vec{e} = \vec{y} - A\vec{x}$
			\item Update the counter: $j = j+1$
		\end{enumerate}
		\item Stop iterating when you’ve done $k$ iterations (i.e. you’ve reached the pre-determined sparsity level of the signal) or when the norm of the residual drops below the threshold $th$. We’ll stop at which ever one happens first.
	\end{itemize}

% \begin{figure} [h!]
% 	\begin{center}
% 	\includegraphics[width=0.5\linewidth]{/Users/Alan/Desktop/invert-or-not}
% 	\caption{Inverting v. Non-Inverting}
% 	\label{fig}
% 	\end{center}
% \end{figure}

\end{itemize}




\noindent{\Large Extra Sanity Checks}
\begin{itemize}
	\item Correlation is not commutative.
	\item The inner product does not depend on the coordinate system the vectors are in, it only depends on the relative angle between these vectors and their length.
	\item For circular cross-correlation, $\Vert \mathbf{C}_{\vec{x}} \vec{y} \Vert =
		\Vert \mathbf{C}_{\vec{y}} \vec{x} \Vert$
	\item We can't always uniquely determine our position in an $n$-dimensional world using $n+1$ beacons (since they might be collinear).
	\item For linear corr, if the signals are periodic, then they continue on to $+\infty$ and $-\infty$, but we’ll just perform the linear correlation over one period, and realize that the signals outside of this period is periodic. For circular correlation, we look at circular shifts of one period of the signal.

	\item Least squares is used for solving an {\color{blue} overdetermined} system and approximating a solution.
	\item In order to use least squares, we need the variable to be {\color{red} linear} in the unknowns.
	\item If the column vectors of $\mathbf{A}$ are linearly dependent, then there could be multiple solutions to the least squares problem.
	\item In linear least squares, the error vector is {\color{red} orthogonal} to Col$(\mathbf{A})$, but it's not the case for any arbitrary cost function.

	\item (Disc) For an orthonormal matrix $\mathbf{A}$, we have $\mathbf{A}^T\mathbf{A} = \mathbf{I}$ and $\mathbf{A}^T = \mathbf{A}^{-1}$
	\begin{itemize}
		\item If $\mathbf{A}\in\mathbb{R}^{N\times N}$, then its columns form a basis for $\mathbb{R}^{N}$ (Prove linear independence and thus invertible).
		\item If $\mathbf{A}\in\mathbb{R}^{N\times M}$ has linearly indpendent columns. The vector $\vec{b}\in\mathbb{R}^N$ is not in the subspace spanned by the columns of $\mathbf{A}$. Then, the projection of $\vec{b}$ onto the subspace spanned by the columns of $\mathbf{A}$ is
		$\text{proj}_{Col(\mathbf{A})} \vec{b} =
			\vec{\tilde{b}} = \mathbf{A} \vec{\tilde{x}} =
			\mathbf{A} (\mathbf{A}^T\mathbf{A})^{-1} \mathbf{A}^T \vec{b}$ \\
		Thus, $\mathbf{A}^T (\vec{b} - \text{proj}_{Col(\mathbf{A})} \vec{b}) =
		\mathbf{A}^T (\vec{b} - \vec{\tilde{b}}) = \vec{0}$ \\
		($\vec{\tilde{b}} - \vec{x}$ is the error vector, which is orthogonal to Col$(\mathbf{A})$).
		\item If $\mathbf{A}\in\mathbb{R}^{N\times M}$ with $N\geq M$, then $\mathbf{A}^T\mathbf{A} = \mathbf{I}_{M\times M}$, which then gives
		$\text{proj}_{Col(\mathbf{A})} \vec{b} = \mathbf{A}\mathbf{A}^T \vec{b}$
		
	\end{itemize}
	\item If given a new cost function, $f(x)$, to be the expression we are trying to minimize.
	\begin{enumerate}
		\item Calculate the derivative.
		\item Set $f'(x) = 0$ and find the critical point.
	\end{enumerate}
	Since the second derivate of $f(x)$ is always positive, so
	$x = \frac{\vec{a}\cdot\vec{b}}{\Vert\vec{a}\Vert^2}$
	is the minimum of $x$ in the 2D case.
	\item For OMP to work, the each code should be “almost” orthogonal to all of the other codes and to all the shifted versions of itself. (Orthogonal means that the inner product of two codes is exactly zero. “Almost” orthogonal means that the magnitude of the inner product is close to zero.) The “gold codes” that we looked at in the homework are good examples of codes that fulfill these properties and would work well for OMP.
	\item OMP: solving underdetermined $A\vec{x} = \vec{b}$ (more unknowns than equations) with {\color{red} sparse} $\vec{x}$.
	\item Matrix-matrix multiplication can be handled in blocks! Care: restrictions on the dimensions of the matrices,
	i.e. $\mathbf{W} \in \mathbb{R}^{n\times m}, \mathbf{Y} \in \mathbb{R}^{m\times l},
	\mathbf{X} \in \mathbb{R}^{n\times p}, \mathbf{Z} \in \mathbb{R}^{p\times l}$.
	User beware: Remember that matrix-matrix multiplication does not, in general, commute.
	$$\Longrightarrow \big[\mathbf{W}\, |\, \mathbf{X}\big] \bigg[\frac{\mathbf{Y}}{\mathbf{Z}}\bigg] = \mathbf{W}\mathbf{Y} + \mathbf{X}\mathbf{Z}$$
	\item Inverse matrices have the same eigenvectors and reciprocal eigenvalues.
\end{itemize}

\end{document}